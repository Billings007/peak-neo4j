rw1d$idx <- 1:100
rw1d %>% gather(key=walker, value=position, -idx) %>% ggplot(aes(x=position, y=idx, col=walker))+geom_line()
rw1d %>% gather(key=walker, value=position, -idx) %>% ggplot(aes(y=position, x=idx, col=walker))+geom_line()
install.packages("xkcd")
library(xkcd)
install.packages("xkcd", dependencies=TRUE)
install.packages("Hmisc")
install.packages("viridis")
devtools::install_github('sjmgarnier/viridis')
install.packages("xkcd", dependencies=TRUE)
library(xkcd)
rw1d %>% gather(key=walker, value=position, -idx) %>% ggplot(aes(y=position, x=idx, col=walker))+geom_line()+theme_xkcd()
rw1d %>% gather(key=walker, value=position, -idx) %>% ggplot(aes(y=position, x=idx, col=walker))+xkcdline()+theme_xkcd()
qt(.035, df=8)
1600-1573.23
#et-up a db connection
library(DBI)
db <- dbConnect(RSQLite::SQLite(), dbname = ":memory:")
?dbConnect
db <- dbConnect(RSQLite::SQLite(), dbname = "/home/jpreszler/gitlab/cs270-s18/data/jacs.sqlite")
dbListTables(db)
dbGetQuery(db, "SELECT DOI FROM Papers")
dbGetQuery(db, "SELECT DOI, COUNT(DISTINCT(paperID)) FROM Papers GROUP BY DOI")
db <- dbConnect(RSQLite::SQLite(), dbname = "/home/jpreszler/gitlab/cs270-s18/data/chinook.db")
dbListTables(db)
dbListFields(db, "sqlite_sequence")
dbListFields(db, "sqlite_stat1")
dbListFields(db, "genres")
dbListFields(db, "tracks")
shiny::runApp(appDir = "/home/jpreszler/gitlab/cs270-s18/SQL_Injection/")
shiny::runApp(appDir = "/home/jpreszler/gitlab/cs270-s18/SQL_Injection/")
shiny::runApp(appDir = "/home/jpreszler/gitlab/cs270-s18/SQL_Injection/")
shiny::runApp(appDir = "/home/jpreszler/gitlab/cs270-s18/SQL_Injection/")
shiny::runApp(appDir = "/home/jpreszler/gitlab/cs270-s18/SQL_Injection/")
?dbGetQuery
shiny::runApp(appDir = "/home/jpreszler/gitlab/cs270-s18/SQL_Injection/")
shiny::runApp(appDir = "/home/jpreszler/gitlab/cs270-s18/SQL_Injection/")
?dbSendQuery
?dbSendStatement
?dbExecute
shiny::runApp(appDir = "/home/jpreszler/gitlab/cs270-s18/SQL_Injection/")
install.packages("XKCDData")
install.packages("XKCDdata")
library(XKCDdata)
print_xkcd(327)
get_xkcd(327)
get_comic(327)
bt <-get_comic(327)
View(bt)
install.packages(c("BH", "DBI", "DT", "RSQLite", "Rcpp", "RcppArmadillo", "RcppEigen", "StanHeaders", "TTR", "XML", "bayesplot", "bindr", "bindrcpp", "blob", "blogdown", "bookdown", "broom", "callr", "caret", "curl", "dbplyr", "ddalpha", "devtools", "digest", "forcats", "forecast", "ggformula", "glmnet", "hms", "htmlwidgets", "httpuv", "igraph", "klaR", "knitr", "lava", "lmtest", "loo", "lubridate", "mapproj", "maps", "matrixStats", "mosaicData", "openssl", "pROC", "packrat", "pillar", "plogr", "prodlim", "psych", "quantmod", "rJava", "randomForest", "readxl", "rgdal", "rlang", "rmarkdown", "rsconnect", "rstantools", "selectr", "servr", "sfsmisc", "sp", "stringi", "stringr", "swirlify", "tibble", "tidyr", "tidyselect", "timeDate", "tseries", "viridis", "withr", "xts", "yaml"))
library(igraph)
p <- c("alex", "robert", "derek", "dominic", "kevin", "franco", "lucas", "andres", "mohammad")
sample(p, 3)
sample(p, 3)
sample(p, 3)
library(igraph)
library(gggraph)
install.packages("ggraph")
library(rvest)
install.packages("rvest")
library(rvest)
acc<-html("http://collegeofidaho.smartcatalogiq.com/current/Undergraduate-Catalog/Courses/ACC-Accounting")
acc<-read_html("http://collegeofidaho.smartcatalogiq.com/current/Undergraduate-Catalog/Courses/ACC-Accounting")
ac
acc
html_nodes(acc)
html_nodes(acc,*)
View(acc)
View(acc)
html_nodes(acc,'div id="main"')
html_table(acc)
html_table("http://collegeofidaho.smartcatalogiq.com/current/Undergraduate-Catalog/Courses/ACC-Accounting")
html_table('http://collegeofidaho.smartcatalogiq.com/current/Undergraduate-Catalog/Courses/ACC-Accounting')
?html_table
?html_node
html_nodes(acc, "div")
html_nodes(acc, "div id=\"main\"")
html_nodes(acc, "ul")
acc_ul_nodes<-html_nodes(acc, "ul")
acc_ul_nodes[1]
View(acc_ul_nodes)
xml_attrs(xml_child(xml_child(acc_ul_nodes[[5]], 1), 1))
xml_attrs(xml_child(xml_child(acc_ul_nodes[[5]], 1), 1))[["href"]]
xml_attrs(xml_child(acc_ul_nodes[[5]], 1))
xml_attrs(xml_child(xml_child(xml_child(acc_ul_nodes[[5]], 1), 1), 1))
subject_url <-"http://collegeofidaho.smartcatalogiq.com/en/current/Undergraduate-Catalog/Courses"
subs <- read_html(subject_url)
subs
subs[2]
subs[[2]]
View(subs)
xml_attrs(xml_child(subs, 2))
xml_attrs(xml_child(xml_child(subs, 2), 1))[["action"]]
html_nodes(subs, ".hasChildren")
subList <- html_nodes(subs, ".hasChildren")
subList[19]
View(subList)
subList[[13:(13+47)]]
subList[13:(13+47)]
install.packages("igraph")
library(rvest)
library(stringr)
library(dplyr)
#url for courses in catalog
base_url <- "http://collegeofidaho.smartcatalogiq.com"
base_url_ext <- "/en/current/Undergraduate-Catalog/Courses"
base_html <- read_html(paste0(base_url,base_url_ext))
#extract links from base page
subjectLinks <- html_nodes(base_html, 'a')
#convert links to text
subjectText <- html_text(subjectLinks)
#pick out corresponds to subject list
#we now have subject codes and names
subjectText <- subjectText[78:120]
subDF <- str_remove_all(subjectText, ' ') %>% str_split(pattern= '-', n=2, simplify=TRUE) %>% as.data.frame()
names(subDF) <- c("sub", "subject")
#get url for subject, for each row in subDF
subDF <-subDF %>% mutate(url= paste0(base_url,base_url_ext,'/',sub,'-',str_replace_all(subject,' ','-')))
library(dplyr)
install.packages("dplyr")
library(dplyr)
subDF <- str_remove_all(subjectText, ' ') %>% str_split(pattern= '-', n=2, simplify=TRUE) %>% as.data.frame()
names(subDF) <- c("sub", "subject")
#get url for subject, for each row in subDF
subDF <-subDF %>% mutate(url= paste0(base_url,base_url_ext,'/',sub,'-',str_replace_all(subject,' ','-')))
View(subDF)
library(purrr)
install.packages("purrr")
acc_url <- subDF$url
acc_links <- html_nodes(read_html(acc_url), 'a')
acc_html <- read_html(acc_url)
acc_url
acc_url <- subDF$url[1]
acc_links <- html_nodes(read_html(acc_url), 'a')
acc_text <- html_text(acc_links)
acc_text
acc_url <- subDF$url[2]
acc_links <- html_nodes(read_html(acc_url), 'a')
acc_text <- html_text(acc_links)
acc_text
library(purrr)
str_detect(acc_text, 'ART-')
acc_text[str_detect(acc_text, 'ART-')]
#url for courses in catalog
base_url <- "http://collegeofidaho.smartcatalogiq.com"
base_url_ext <- "/en/current/Undergraduate-Catalog/Courses"
base_html <- read_html(paste0(base_url,base_url_ext))
#extract links from base page
subjectLinks <- html_nodes(base_html, 'a')
library(rvest)
library(stringr)
library(dplyr)
library(purrr)
#url for courses in catalog
base_url <- "http://collegeofidaho.smartcatalogiq.com"
base_url_ext <- "/en/current/Undergraduate-Catalog/Courses"
base_html <- read_html(paste0(base_url,base_url_ext))
#extract links from base page
subjectLinks <- html_nodes(base_html, 'a')
#convert links to text
subjectText <- html_text(subjectLinks)
#pick out what corresponds to subject list
#we now have subject codes and names
subjectText <- subjectText[78:120]
subDF <- str_split(pattern= '-', n=2, simplify=TRUE) %>% str_trim(side="both") %>% as.data.frame()
subDF <- str_split(subjectText,pattern= '-', n=2, simplify=TRUE) %>% str_trim(side="both") %>% as.data.frame()
names(subDF) <- c("sub", "subject")
View(subDF)
subDF <- str_split(subjectText,pattern= '-', n=2, simplify=TRUE) %>% as.data.frame()
names(subDF) <- c("sub", "subject")
subDF <-mutate(subDF, sub=str_trim(sub, side="both"), subject=str_trim(subject, side="both"))
#get url for subject, for each row in subDF
subDF <-subDF %>% mutate(url= paste0(base_url,base_url_ext,'/',sub,'-',str_replace_all(subject,' ','-')))
get_class_list <- function(i){
#get list of links on subject page
class_links <- html_nodes(read_html(subDF$url[i]), 'a')
#turn links to text
class_list <- html_text(class_links)
#only keep links for classes, each subject has
#classes starting in a different position
class_list <- class_list[str_detect(class_list, paste0(subDF$sub[i],'-'))]
#now build class dataframe with sub,number,url - we'll get the
#name and other details on the next scrape
return(class_list)
}
map_df(1:length(subDF$sub), get_class_list)
get_class_list(3)
get_class_list(4)
get_class_list(5)
get_class_list <- function(i){
#get list of links on subject page
class_links <- html_nodes(read_html(subDF$url[i]), 'a')
#turn links to text
class_list <- html_text(class_links)
#only keep links for classes, each subject has
#classes starting in a different position
class_list <- class_list[str_detect(class_list, paste0(subDF$sub[i],'-'))]
#now build class dataframe with sub,number,url - we'll get the
#name and other details on the next scrape
print(head(class_list))
return(class_list)
}
map_df(1:length(subDF$sub), get_class_list)
#hack to fix ENV link issue
envStud_url <- subDF[subDF$sub=='ENV',]$url
library(rvest)
library(stringr)
library(dplyr)
library(purrr)
#url for courses in catalog
base_url <- "http://collegeofidaho.smartcatalogiq.com"
base_url_ext <- "/en/current/Undergraduate-Catalog/Courses"
base_html <- read_html(paste0(base_url,base_url_ext))
#extract links from base page
subjectLinks <- html_nodes(base_html, 'a')
#convert links to text
subjectText <- html_text(subjectLinks)
#pick out what corresponds to subject list
#we now have subject codes and names
subjectText <- subjectText[78:120]
subDF <- str_split(subjectText,pattern= '-', n=2, simplify=TRUE) %>% as.data.frame()
names(subDF) <- c("sub", "subject")
subDF <-mutate(subDF, sub=str_trim(sub, side="both"), subject=str_trim(subject, side="both"))
#get url for subject, for each row in subDF
subDF <-subDF %>% mutate(url= paste0(base_url,base_url_ext,'/',sub,'-',str_replace_all(subject,' ','-')))
#hack to fix ENV link issue
envStud_url <- subDF[subDF$sub=='ENV',]$url
subDF[subDF$sub=='ENV',]$url <- str_replace(envStud_url, 'Studies', 'Science')
get_class_list <- function(i){
#get list of links on subject page
class_links <- html_nodes(read_html(subDF$url[i]), 'a')
#turn links to text
class_list <- html_text(class_links)
#only keep links for classes, each subject has
#classes starting in a different position
class_list <- class_list[str_detect(class_list, paste0(subDF$sub[i],'-'))]
#now build class dataframe with sub,number,url - we'll get the
#name and other details on the next scrape
print(head(class_list)) #for testing to see when a suject fails
return(class_list)
}
temp <- map_dfr(1:length(subDF$sub), get_class_list)
length(subDF$sub)
get_class_list <- function(i){
#get list of links on subject page
class_links <- html_nodes(read_html(subDF$url[i]), 'a')
#turn links to text
class_list <- html_text(class_links)
#only keep links for classes, each subject has
#classes starting in a different position
class_list <- class_list[str_detect(class_list, paste0(subDF$sub[i],'-'))]
#now build class dataframe with sub,number,url - we'll get the
#name and other details on the next scrape
print(i) #for testing to see when a suject fails
return(class_list)
}
temp <- map_dfr(1:length(subDF$sub), get_class_list)
temp <- map_df(1:length(subDF$sub), get_class_list)
?map_df
get_class_list <- function(i){
#get list of links on subject page
class_links <- html_nodes(read_html(subDF$url[i]), 'a')
#turn links to text
class_list <- html_text(class_links)
#only keep links for classes, each subject has
#classes starting in a different position
class_list <- class_list[str_detect(class_list, paste0(subDF$sub[i],'-'))]
#now build class dataframe with sub,number,url - we'll get the
#name and other details on the next scrape
print(i) #for testing to see when a suject fails
return(as.data.frame(class_list))
}
temp <- map_df(1:length(subDF$sub), get_class_list)
warnings()
View(temp)
get_class_list <- function(i){
#get list of links on subject page
class_links <- html_nodes(read_html(subDF$url[i]), 'a')
#turn links to text
class_list <- html_text(class_links)
#only keep links for classes, each subject has
#classes starting in a different position
class_list <- class_list[str_detect(class_list, paste0(subDF$sub[i],'-'))]
#now build class dataframe with sub,number,url - we'll get the
#name and other details on the next scrape
#We'll split on 1st space, discard everything after it and use
#what's before it to build the required DF
class_list <- str_split(class_list, " ", n=2, simplify=TRUE) %>% data.frame()
print(i) #for testing to see when a suject fails
return(class_list)
}
temp <- map_df(1:length(subDF$sub), get_class_list)
View(temp)
write.csv(classes, "/home/jpreszler/github_web/peak-neo4j/data/classes-catalog-2017.csv", row.names = FALSE)
library(rvest)
library(stringr)
library(dplyr)
library(purrr)
library(tidyr)
#url for courses in catalog
base_url <- "http://collegeofidaho.smartcatalogiq.com"
base_url_ext <- "/en/current/Undergraduate-Catalog/Courses"
base_html <- read_html(paste0(base_url,base_url_ext))
#extract links from base page
subjectLinks <- html_nodes(base_html, 'a')
#convert links to text
subjectText <- html_text(subjectLinks)
sub_url <- html_attr(subjectLinks, 'href')[78:120]
#pick out what corresponds to subject list
#we now have subject codes and names
subjectText <- subjectText[78:120]
subDF <- str_split(subjectText,pattern= '-', n=2, simplify=TRUE) %>% as.data.frame()
names(subDF) <- c("sub", "subject")
subDF <-mutate(subDF, sub=str_trim(sub, side="both"), subject=str_trim(subject, side="both"))
#get url for subject, for each row in subDF
#subDF <-subDF %>% mutate(url= paste0(base_url,base_url_ext,'/',sub,'-',str_replace_all(subject,' ','-')))
subDF <- mutate(subDF, url=paste0(base_url,sub_url))
#hack to fix ENV link issue -- not needed since url is pulled from html_attr
#envStud_url <- subDF[subDF$sub=='ENV',]$url
#subDF[subDF$sub=='ENV',]$url <- str_replace(envStud_url, 'Studies', 'Science')
get_class_list <- function(i){
#get list of links on subject page
class_links <- html_nodes(read_html(subDF$url[i]), 'a')
#turn links to text
class_list <- html_text(class_links)
class_url <- html_attr(class_links, 'href')
classDF <- data.frame(list=class_list, url=class_url)
#only keep links for classes, each subject has
#classes starting in a different position
classDF <- classDF %>% filter(str_detect(list, paste0(subDF$sub[i],'-')))
#now build class dataframe with sub,number,name,url - we'll get the
#other details on the next scrape
#We'll split on 1st space, discard everything after it and use
#what's before it to build the required DF
classDF <- separate(classDF, list, into=c("id", "name"),sep=" ",extra="merge")
classDF <- separate(classDF, id, into=c("sub", "number"), sep="-")
#the id field has the last part of the new url, we need the
#subject url with the course level (100,200,etc) then id
classDF <- mutate(classDF, url=paste0(base_url,url))
return(classDF)
}
classes <- map_dfr(1:length(subDF$sub), get_class_list)
write.csv(classes, "/home/jpreszler/github_web/peak-neo4j/data/classes-catalog-2017.csv", row.names = FALSE)
install.packages("tidyr")
library(rvest)
library(stringr)
library(dplyr)
library(purrr)
library(tidyr)
#url for courses in catalog
base_url <- "http://collegeofidaho.smartcatalogiq.com"
base_url_ext <- "/en/current/Undergraduate-Catalog/Courses"
base_html <- read_html(paste0(base_url,base_url_ext))
#extract links from base page
subjectLinks <- html_nodes(base_html, 'a')
#convert links to text
subjectText <- html_text(subjectLinks)
sub_url <- html_attr(subjectLinks, 'href')[78:120]
#pick out what corresponds to subject list
#we now have subject codes and names
subjectText <- subjectText[78:120]
subDF <- str_split(subjectText,pattern= '-', n=2, simplify=TRUE) %>% as.data.frame()
names(subDF) <- c("sub", "subject")
subDF <-mutate(subDF, sub=str_trim(sub, side="both"), subject=str_trim(subject, side="both"))
#get url for subject, for each row in subDF
#subDF <-subDF %>% mutate(url= paste0(base_url,base_url_ext,'/',sub,'-',str_replace_all(subject,' ','-')))
subDF <- mutate(subDF, url=paste0(base_url,sub_url))
#hack to fix ENV link issue -- not needed since url is pulled from html_attr
#envStud_url <- subDF[subDF$sub=='ENV',]$url
#subDF[subDF$sub=='ENV',]$url <- str_replace(envStud_url, 'Studies', 'Science')
get_class_list <- function(i){
#get list of links on subject page
class_links <- html_nodes(read_html(subDF$url[i]), 'a')
#turn links to text
class_list <- html_text(class_links)
class_url <- html_attr(class_links, 'href')
classDF <- data.frame(list=class_list, url=class_url)
#only keep links for classes, each subject has
#classes starting in a different position
classDF <- classDF %>% filter(str_detect(list, paste0(subDF$sub[i],'-')))
#now build class dataframe with sub,number,name,url - we'll get the
#other details on the next scrape
#We'll split on 1st space, discard everything after it and use
#what's before it to build the required DF
classDF <- separate(classDF, list, into=c("id", "name"),sep=" ",extra="merge")
classDF <- separate(classDF, id, into=c("sub", "number"), sep="-")
#the id field has the last part of the new url, we need the
#subject url with the course level (100,200,etc) then id
classDF <- mutate(classDF, url=paste0(base_url,url))
return(classDF)
}
classes <- map_dfr(1:length(subDF$sub), get_class_list)
write.csv(classes, "/home/jpreszler/github_web/peak-neo4j/data/classes-catalog-2017.csv", row.names = FALSE)
library(rvest)
library(stringr)
library(dplyr)
library(purrr)
library(tidyr)
#url for courses in catalog
base_url <- "http://collegeofidaho.smartcatalogiq.com"
base_url_ext <- "/en/current/Undergraduate-Catalog/Courses"
base_html <- read_html(paste0(base_url,base_url_ext))
#extract links from base page
subjectLinks <- html_nodes(base_html, 'a')
#convert links to text
subjectText <- html_text(subjectLinks)
sub_url <- html_attr(subjectLinks, 'href')[78:120]
#pick out what corresponds to subject list
#we now have subject codes and names
subjectText <- subjectText[78:120]
subDF <- str_split(subjectText,pattern= '-', n=2, simplify=TRUE) %>% as.data.frame()
names(subDF) <- c("sub", "subject")
subDF <-mutate(subDF, sub=str_trim(sub, side="both"), subject=str_trim(subject, side="both"))
#get url for subject, for each row in subDF
#subDF <-subDF %>% mutate(url= paste0(base_url,base_url_ext,'/',sub,'-',str_replace_all(subject,' ','-')))
subDF <- mutate(subDF, url=paste0(base_url,sub_url))
#hack to fix ENV link issue -- not needed since url is pulled from html_attr
#envStud_url <- subDF[subDF$sub=='ENV',]$url
#subDF[subDF$sub=='ENV',]$url <- str_replace(envStud_url, 'Studies', 'Science')
get_class_list <- function(i){
#get list of links on subject page
class_links <- html_nodes(read_html(subDF$url[i]), 'a')
#turn links to text
class_list <- html_text(class_links)
class_url <- html_attr(class_links, 'href')
classDF <- data.frame(list=class_list, url=class_url)
#only keep links for classes, each subject has
#classes starting in a different position
classDF <- classDF %>% filter(str_detect(list, paste0(subDF$sub[i],'-')))
#now build class dataframe with sub,number,name,url - we'll get the
#other details on the next scrape
#We'll split on 1st space, discard everything after it and use
#what's before it to build the required DF
classDF <- separate(classDF, list, into=c("id", "name"),sep=" ",extra="merge")
classDF <- separate(classDF, id, into=c("sub", "number"), sep="-")
#the id field has the last part of the new url, we need the
#subject url with the course level (100,200,etc) then id
classDF <- mutate(classDF, url=paste0(base_url,url))
return(classDF)
}
classes <- map_dfr(1:length(subDF$sub), get_class_list)
write.csv(classes, "classes-catalog-2017.csv", row.names = FALSE)
View(classes)
View(classes)
pwd()
setwd("~/github-web/peak-neo4j/data")
write.csv(classes, "class-list-2017-2018.csv", row.names = FALSE)
group_by(classes, sub)%>%summarise(subject=sub, count=n())
group_by(classes, sub)%>%summarise(sub, count=n())
group_by(classes, sub)%>%summarise(count=n())
group_by(classes, sub)%>%summarise(count=n()) %>% View()
library(rvest)
library(stringr)
library(dplyr)
library(purrr)
library(tidyr)
#url for courses in catalog
base_url <- "http://collegeofidaho.smartcatalogiq.com"
base_url_ext <- "/en/current/Undergraduate-Catalog/Courses"
base_html <- read_html(paste0(base_url,base_url_ext))
#extract links from base page
subjectLinks <- html_nodes(base_html, 'a')
#convert links to text
subjectText <- html_text(subjectLinks)
sub_url <- html_attr(subjectLinks, 'href')[78:120]
#pick out what corresponds to subject list
#we now have subject codes and names
subjectText <- subjectText[78:120]
subDF <- str_split(subjectText,pattern= '-', n=2, simplify=TRUE) %>% as.data.frame()
names(subDF) <- c("sub", "subject")
subDF <-mutate(subDF, sub=str_trim(sub, side="both"), subject=str_trim(subject, side="both"))
#get url for subject, for each row in subDF
#subDF <-subDF %>% mutate(url= paste0(base_url,base_url_ext,'/',sub,'-',str_replace_all(subject,' ','-')))
subDF <- mutate(subDF, url=paste0(base_url,sub_url))
#hack to fix ENV link issue -- not needed since url is pulled from html_attr
#envStud_url <- subDF[subDF$sub=='ENV',]$url
#subDF[subDF$sub=='ENV',]$url <- str_replace(envStud_url, 'Studies', 'Science')
get_class_list <- function(i){
#get list of links on subject page
class_links <- html_nodes(read_html(subDF$url[i]), 'a')
#turn links to text
class_list <- html_text(class_links)
class_url <- html_attr(class_links, 'href')
classDF <- data.frame(list=class_list, url=class_url)
#only keep links for classes, each subject has
#classes starting in a different position
classDF <- classDF %>% filter(str_detect(list, paste0(subDF$sub[i],'-')))
#now build class dataframe with sub,number,name,url - we'll get the
#other details on the next scrape
#We'll split on 1st space, discard everything after it and use
#what's before it to build the required DF
classDF <- separate(classDF, list, into=c("id", "name"),sep=" ",extra="merge")
#two theater classes have typos -THE-###
#this is solely dealing with that
classDF$id <- str_replace(classDF$id, "-THE", "THE")
#back to normal
classDF <- separate(classDF, id, into=c("sub", "number"), sep="-")
#the id field has the last part of the new url, we need the
#subject url with the course level (100,200,etc) then id
classDF <- mutate(classDF, url=paste0(base_url,url))
return(classDF)
}
classes <- map_dfr(1:length(subDF$sub), get_class_list)
group_by(classes, sub)%>%summarise(count=n()) %>% View()
write.csv(classes, "class-list-2017-2018.csv", row.names = FALSE)
classes %>% mutate(level = paste0(number[1],"00"))
str_extract(classes$number, '[:digit:]')
classes %>% mutate(level=paste0(str_extract(number, '[:digit:]'),"00"))
classes <- classes %>% mutate(level=paste0(str_extract(number, '[:digit:]'),"00"))
group_by(classes, sub, level)%>%summarise(count=n()) %>% View()
install.packages("knitr")
install.packages("knitr")
